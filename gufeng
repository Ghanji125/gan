# -*- encoding=utf-8 -*-
import requests
from bs4 import BeautifulSoup as bs
from urllib.request import urlretrieve
import os
import time

def url_request(url):
    response = requests.get(url, timeout=20)
    if str(response) == "<Response [200]>":   #response为requests.models.Response数据类型
        web_html = bs(response.text,"lxml")
        return web_html
    else:
        return 0

def get_url(bs_tag):
    url_ahead_part = "https://www.gufengmh8.com"
    key = bs_tag.text.strip()
    url_last_part = bs_tag.attrs["href"]
    url = url_ahead_part + url_last_part
    return key,url

def bs_find(web_html):
    div_content_list = web_html.find_all("div", class_ = "contents-section-inner")  #10800之前内容转移链接至其他网页
    img_list = div_content_list[0].find_all("img")
    return img_list
def extract_ref_num(bs_tag):
    num_1 = bs_tag.find_all("script")[2].text.find("images/comic") + 12
    num_2 = bs_tag.find_all("script")[2].text.find('"', num1)
    mid_part = bs_tag.find_all("script")[2].text[num1:num2]
    return mid_part

def combine_img_link(mid_part,jpg):
    first_part = "https://res.gufengmh8.com/images/comic"
    img_url = first_part + mid_part + jpg
    return img_url

def main():
    """
        Download
    """
    base_url = "https://www.gufengmh8.com/manhua/jieliyaosheng/"
    base_html_web = url_request(base_url)
    while base_html_web:
        chapter_list = base_html_web.find_all("ul",id="chapter-list-1")
        web_list = bs(str(chapter_list)).find_all("a")
        web_dict = {}
        for i in range(28,len(web_list)):
            key, value = get_url(web_list[i])
            web_dict[key] = value
        del web_dict["你愿意为梦想付费吗"]
        del web_dict["1月28号请假条"]
        for k,v in web_dict.items():
            os.makedirs(r'D:\GuFeng\JieLiYaoSheng\{}'.format(k), exist_ok=True)
            sub_html_web = url_request(v)
            while sub_html_web:
                mid_part = extract_ref_num(sub_html_web)
                r = re.compile(r'\[".*\]')
                img_list = r.findall(sub_html_web.)
                jpg_list = img_list[0].split('","')
                
        mid_part = 
        img_url = combine_img_link(mid_part,jpg)
        urlretrieve(img_url,r'D:\GuFeng\JieLiYaoSheng\{}\{}.jpg'.format(save_dir,j))
        
       
        
    else:
        pass
    
if __name__ == "__main__":
    main()

#################################################################################################################

# -*- encoding=utf-8 -*-
import requests
from bs4 import BeautifulSoup as bs
from urllib.request import urlretrieve
import os
import time

def url_request(url):
    response = requests.get(url, timeout=20)
    if str(response) == "<Response [200]>":   #response为requests.models.Response数据类型
        web_html = bs(response.text,"lxml")
        return web_html
    else:
        return 0

def get_url(bs_tag):
    url_ahead_part = "https://www.gufengmh8.com"
    key = bs_tag.text.strip()
    url_last_part = bs_tag.attrs["href"]
    url = url_ahead_part + url_last_part
    return key,url

def find_extract(string,pattern_1,pattern_2):
    start = string.find(pattern_1) + len(pattern_1)
    end = string.find(pattern_2, start)
    if end == -1:
        return 0
    else:
        return string[start:end]

response = requests.get("https://www.gufengmh8.com/manhua/jieliyaosheng/",timeout=10)
web_html = bs(response.text,"lxml")
chapter_list = web_html.find_all("ul",id="chapter-list-1")
web_list = bs(str(chapter_list)).find_all("a")
web_dict = {}
for i in range(28,len(web_list)):
    key, value = get_url(web_list[i])
    web_dict[key] = value
del web_dict["你愿意为梦想付费吗"]
del web_dict["1月28号请假条"]

web_dict_key_list = list(web_dict.keys())

for i in range(len(web_dict_key_list)):
    num = web_dict_key_list[i]
    dir_name = num.split()[0]
    os.makedirs("./gufeng/{}".format(dir_name), exist_ok=True)
    sub_web_url = web_dict[num]
    sub_web_html = url_request(sub_web_url)
    content_string = sub_web_html.find_all("script")[2].text
    first_part = "https://res.gufengmh8.com/"
    mid_part = find_extract(content_string, 'chapterPath = "', '"')
    last_part = find_extract(content_string, 'chapterImages = ["', '"]')
    last_part_list = last_part.split('","')
    for j in range(len(last_part_list)):
        img_url = first_part + mid_part + last_part_list[j]
        urlretrieve(img_url,"./gufeng/{}/{}.jpg".format(dir_name,i))
    time.sleep(5)
